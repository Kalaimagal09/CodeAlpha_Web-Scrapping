{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ashok kumar\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ashok kumar\\anaconda3\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ashok kumar\\anaconda3\\lib\\site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ashok kumar\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashok kumar\\anaconda3\\lib\\site-packages (from requests) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ashok kumar\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\ashok kumar\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! We have downloaded the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://books.toscrape.com/\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Success! We have downloaded the webpage.\")\n",
    "\n",
    "    response.text \n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 books on this page.\n"
     ]
    }
   ],
   "source": [
    "book_containers = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "print(f\"Found {len(book_containers)} books on this page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted data. Here's a sample:\n",
      "[{'Title': 'A Light in the Attic', 'Price': 'Â£51.77', 'Rating': 'Three'}, {'Title': 'Tipping the Velvet', 'Price': 'Â£53.74', 'Rating': 'One'}, {'Title': 'Soumission', 'Price': 'Â£50.10', 'Rating': 'One'}]\n"
     ]
    }
   ],
   "source": [
    "all_books_data = []\n",
    "\n",
    "for book in book_containers:\n",
    "    title = book.h3.a['title']\n",
    "    \n",
    "    price = book.find('p', class_='price_color').text\n",
    "    \n",
    "    rating = book.find('p', class_='star-rating')['class'][1]\n",
    "    \n",
    "    book_data = {\n",
    "        'Title': title,\n",
    "        'Price': price,\n",
    "        'Rating': rating\n",
    "    }\n",
    "    \n",
    "    all_books_data.append(book_data)\n",
    "\n",
    "print(\"Successfully extracted data. Here's a sample:\")\n",
    "print(all_books_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the scraper...\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-1.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-2.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-3.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-4.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-5.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-6.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-7.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-8.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-9.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-10.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-11.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-12.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-13.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-14.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-15.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-16.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-17.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-18.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-19.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-20.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-21.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-22.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-23.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-24.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-25.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-26.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-27.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-28.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-29.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-30.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-31.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-32.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-33.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-34.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-35.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-36.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-37.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-38.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-39.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-40.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-41.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-42.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-43.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-44.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-45.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-46.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-47.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-48.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-49.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-50.html\n",
      "Scraping complete!\n",
      "Data has been saved to books_data.csv\n",
      "Total books scraped: 1000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"http://books.toscrape.com/catalogue/\"\n",
    "current_page_url = base_url + \"page-1.html\"\n",
    "\n",
    "all_books_data = []\n",
    "\n",
    "print(\"Starting the scraper...\")\n",
    "\n",
    "while current_page_url:\n",
    "    response = requests.get(current_page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    book_containers = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    print(f\"Scraping page: {current_page_url}\")\n",
    "\n",
    "    for book in book_containers:\n",
    "        title = book.h3.a['title']\n",
    "        price = book.find('p', class_='price_color').text\n",
    "        rating = book.find('p', class_='star-rating')['class'][1]\n",
    "        \n",
    "        book_data = {\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Rating': rating\n",
    "        }\n",
    "        all_books_data.append(book_data)\n",
    "\n",
    "    next_button = soup.find('li', class_='next')\n",
    "    \n",
    "    if next_button:\n",
    "        next_page_relative_url = next_button.a['href']\n",
    "        current_page_url = base_url + next_page_relative_url\n",
    "    else:\n",
    "    \n",
    "        current_page_url = None\n",
    "\n",
    "print(\"Scraping complete!\")\n",
    "\n",
    "df = pd.DataFrame(all_books_data)\n",
    "\n",
    "df.to_csv('books_data.csv', index=False)\n",
    "\n",
    "print(\"Data has been saved to books_data.csv\")\n",
    "print(f\"Total books scraped: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
